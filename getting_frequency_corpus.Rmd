---
title: "Get freqs for NWR YD project"
author: "AC"
date: "9/8/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Initial clean up

This is a better than what we had in divime, because it does more cleanup. To run it, you need to be in raw_YEL

```{bash, eval=F}
grep "^[FM]A" txt/*.txt | # use only adult speech
cut -f 6 | #take transcription
sed "s/\[: /\[:_/g" |  sed "s/\[=! /\[:_/g" | sed "s/\[- /\[-_/g" | #remove spaces that are not real ones
grep -v "\[-_" | #remove sentences in Eng and Pidgin
 grep -v "he " | grep -vw "they" | grep -vw "I"  | grep -vw "you" | grep -vw "your"| grep -vw "his" | grep -vw "hey" | grep -vw "going" | grep -vw "people" | grep -vw "dogs" | grep -vw "puppies" | grep -vw "boys" | grep -vw "and"| grep -vw "container" | grep -vw "brother" | grep -vw "good"| grep -vw "where"| grep -vw "umbrella"| grep -vw "hello"| grep -vw "here" | grep -vw "this"| grep -vw "married" | grep -vw "here" | grep -vw "okay"| grep -vw "nice"| grep -vw "baked"| grep -vw "singsing"| grep -vw "cooked"| grep -vw "banana"| grep -vw "hurry"| grep -vw "fire"| grep -vw "about" | #there were still some English sentences
grep -vw "alright" |grep -vw "already" |grep -vw "almost" |grep -vw "another" |
grep -vw "bye" |grep -vw "calling" |grep -vw "coming" |grep -vw "clothes" |
grep -vw "complete" |grep -vw "counting" |grep -vw "enough" |grep -vw "fight" |
grep -vw "first" |grep -vw "finished" |grep -vw "there"|grep -vw "which" |grep -vw "witchcraft" |grep -vw "small"|grep -vw "right"|grep -vw "ready"|grep -vw "out"|grep -vw "our"|grep -vw "later"|grep -vw "leave"|grep -vw "eat"|grep -vw "everything"|grep -vw "books"|
grep -vw "window" |grep -vw "want"|grep -vw "sister"|grep -vw "sleeping"|
grep -vw "sometimes" | grep -vw "somehwere"|grep -vw "scared"|grep -vw "saw" |
grep -vw "inside" | grep -vw "gonna" | grep -vw "her" | grep -vw "him"|
grep -vw "here" | grep -vw "easy" | grep -vw "early" | grep -vw "eye"|
grep -vw "cook" | grep -vw "dog" | grep -vw "area" | grep -vw "around"|
grep -v "a@l" | grep -v "e@l" | grep -v "d@l"|
sed 's/\[[^[]*\]//g' | #delete comments
tr ' ' '\n' | #cut at word boundaries
tr -d '?' | tr -d '.' | tr -d '!' | tr -d '-' | tr -d ',' | #clean up punctuation
grep -v "&" | grep -v "@s" | #remove switches
grep -v "xxx" | grep -vw "xx" | grep -vw "hm" | grep -vw "mm" | grep -vw "mmhm" | grep -vw "oh" | grep -vw "yeah" | grep -v "uhhuh" | grep -v "ho" | grep -v "mmmm" | grep -v "chuk"| grep -v "ha"| grep -v "ah" | grep -v "ehheh" | grep -v "hmmm" |#remove nonwords
grep -v "[A-Z]" | grep -vw "Ńaamońo" | #get rid of all names
sed "s/aa+/aa/g" | sed "s/ee+/ee/g" | sed "s/ii+/ii/g" | sed "s/oo+/oo/g" | sed "s/uu+/aa/g" |
sed "s/êê+/êê/g" | sed "s/ââ+/ââ/g" | sed "s/áá+/áá/g" | sed "s/óó+/óó/g" | #exaggeration of vowel length
sed "s/aaaa/aa/g" |
tr -d '>' | tr -d '<' |tr -d '(' |tr -d ')' | sed "s/@c//g" | # final cleaning and write out
sed "s/che/te/g" |  sed "s/chi/ti/g" | sort | grep -v "[0-9]" | sed '/^$/d' > words_corpus.txt


```

## Getting frequencies for segments in our stimuli 

Middy gave me an onset list that converts all vowels and consonants to common representations for the purposes of counting syllables (rossel-ortho-replacements.txt). I thought of the following changes:

- remove all rewrites for vowels
- but that destroys the context for the following rules, eg not removing colon means knw does not find a match in knw:a 
- but why weren't these defined with regular expressions anyway?
- it looks like I should be careful with the 4- and 3-letter phonemes, because I do not have those in my rewrites but not otherwise

So all things considered, it looks like it may be easier to just do the replacement of the 4- and 3-letter segments here, just being careful to map these to something that is not used in the stimuli, such as ngm.

Also, this reveals my rewrites were incomplete, so I added some lines for the onsets that were missing in my initial correspondance list.

The correspondances here looks similar to the one in wrangling but it has more entries because we want to separate each phoneme, and we want to distinguish between short and long

```{r pressure, echo=FALSE}

correspondances=matrix( #list correspondances always by pairs, orthography then phonology
  c(
    #nasal
    ":ii","1 ",
    ":ee","2 ",
    ":aa","3 ",
    ":êê","4 ",
    ":ââ","5 ",
    ":uu","6 ",
    ":oo","7 ",
    ":óó","37 ",
    
    
    ":i","8 ",
    ":e","9 ",
    ":a","0 ",
    ":ê","10 ",
    ":â","11 ",
    ":u","12 ",
    ":o","13 ",
     ":ó","38 ",
   
    #non-nasal
    "îî","14 ",
    "êê","15 ",
    "ââ","16 ",
    "éé","17 ",
    "áá","18 ",
    "óó","19 ",
    
    "aa","20 ",
    "ee","21 ",
    "ii","22 ",
    "oo","23 ",
    "uu","24 ",

    "î","25 ",
    "ê","26 ",
    "â","27 ",
    "é","28 ",
    "á","29 ",
    "ó","30 ",
    
    "a","31 ",
    "e","32 ",
    "i","33 ",
    "o","34 ",
    "u","35 ",
    "ú","36 ",
    #consonants from rossel ortho replacements, missing previously
    "mbyw","39 ",
    "mbwy","40 ",
    "pwy","41 ",
    "pyw","42 ",
    "tpy","43 ",
    "dpy","44 ",
    "kpy","45 ",
    "myw","46 ",
    "mwy","47 ",
    "ngw","48 ",
    "nmy","49 ",
    "ngm","50 ",
    "mby","51 ",
    "mbw","52 ",
    "nty","53 ",
    "ndy","54 ",
    "nkw","55 ",
    "mty","56 ",
    "mdy","57 ",
    "mgw","58 ",
    "dny","59 ",
    "dmy","60 ",
    "knw","61 ",
    "py","62 ",
    "pw","63 ",
    "ty","64 ",
   # "ch"
    "dy","65 ",
    "ky","66 ",
    "kw","67 ",
    #"tp"
    #"dp"
    #"kp"
    "my","68 ",
    "mw","69 ",
    "ny","70 ",
    #"ng"
    "nm","71 ",
    #"mb"
    #"nt"
    "nj","72 ",
    #"nd"
    #"nk"
    #"mt"
    #"md"
    #"mg"
    #"dn"
    #"dm"
   # "kn"
    #"km"
    #"vy"
    "ly","73 ",
    #"lv"
    #"gh"
    #consonants, veeeery dirty!
    "tp","Tp ",
    "dp","Dp ",
    "kp","Kp ",
    "ngm","Ngm ",
    "ńm","ñ ",
    "ng","Ng ",
    "mb","Mb ",
    "nd","Nd ",
    "nt","Nt ",
    "nk","Nk ",
    "dn","Dn ",
    "kn","Kn ",
    "vy","Vy ",
    "mt","Mt ",
    "md","Md ",
    "mg","Mg ",
    "dm","Dm ",
    "km","Km ",
    "gh","Gh ",
    "lv","Lv ",
    "cch","CCh ",
    "'n","'n ",
    "ń","ń ",
    "ch","Ch ", #NOTE
   "w","w ",
   "ẃ","ẃ ",
   "r","r ",
   "t","t ",
   "y","y ",
   "p","p ",
   "s","s ",
   "d","d ",
  "f","f ",
   "g","g ",
   "h","h ",
   "j","j ",
   "k","k ",
   "l","l ",
   "z","z ",
   "c","c ",
   "v","v ",
   "b","b ",
   "n","n ",
   "m","m "
    
  ),#last item above should not have a comma
  ncol = 2,byrow = T)


```

```{r token freq}
# get frequencies in raw_YEL

scan("words_corpus.txt",what="char")->wds

#wds[1:1000]

#initialize the unichar phono-like representation
wds_uni=wds

for(i in 1:dim(correspondances)[1]) { #transform into unicharacter
  wds_uni=gsub(correspondances[i,1],correspondances[i,2],wds_uni,fixed=T,useBytes = T)
}
#wds_uni[1:1000]

wds_uni=unlist(strsplit(wds_uni,split=" "))
counts=data.frame(table(wds_uni))
counts$wds_uni=as.character(counts$wds_uni)

correspondances2=correspondances
colnames(correspondances2)<-c("ortho","fake")
correspondances2=data.frame(correspondances2)
correspondances2$fake=gsub(" ","",as.character(correspondances2$fake))

#backtranslate
merge(counts,correspondances2,by.x="wds_uni",by.y="fake")->counts
counts=counts[order(counts$Freq),]

colnames(counts)[2]<-"counts_corpus"
counts$freq_corpus=counts$counts_corpus/sum(counts$counts_corpus)

write.table(counts,"segment-counts.txt",col.names = T,row.names = F,quote=T,sep="\t")
```

```{r type freq}

#initialize the unichar phono-like representation
wds_uni= names(table(wds))

for(i in 1:dim(correspondances)[1]) { #transform into unicharacter
  wds_uni=gsub(correspondances[i,1],correspondances[i,2],wds_uni,fixed=T,useBytes = T)
}
#wds_uni[1:1000]

wds_uni=unlist(strsplit(wds_uni,split=" "))
counts=data.frame(table(wds_uni))
counts$wds_uni=as.character(counts$wds_uni)

colnames(correspondances)<-c("ortho","fake")
correspondances=data.frame(correspondances)
correspondances$fake=gsub(" ","",as.character(correspondances$fake))

#backtranslate
merge(counts,correspondances,by.x="wds_uni",by.y="fake")->counts
counts=counts[order(counts$Freq),]

colnames(counts)[2]<-"counts_corpus_types"
counts$freq_corpus_types=counts$counts_corpus/sum(counts$counts_corpus)

write.table(counts,"segment-counts-types.txt",col.names = T,row.names = F,quote=T,sep="\t")
```

## Analyses

I ran the code in wrangling again, to integrate these counts & freqs into the main data set.

In addition, I wrote out a new version of segments which has the corpus frequency

```{r}
read.table("segments_with_cor_freq.txt",header=T)->segments # not ran
plot(segments$pc~segments$freq_corpus,type="n")
text(segments$pc~segments$freq_corpus,labels=segments$ortho)

plot(segments$pc~log(segments$freq_corpus),type="n")
text(segments$pc~log(segments$freq_corpus),labels=segments$ortho)

#some stimuli sounds do not occur in the corpus at all
#so we'll give them a really small frequency, just so that they show up
#and we tag them in red
segments$freq_corpus[is.na(segments$freq_corpus)]<-.0001
plot(segments$pc~log(segments$freq_corpus),type="n",main="blue fitted to sounds in corpus, purple to all")
text(segments$pc~log(segments$freq_corpus),labels=segments$ortho,col=ifelse(segments$freq_corpus<.00057,"red","blue"))
#abline(lm(segments$pc~log(segments$freq_corpus),subset=c(segments$freq_corpus<.00057)),col="red",lty=2) #this one is impossible, bc it's vertical, all the x are the same
abline(lm(segments$pc~log(segments$freq_corpus),subset=c(segments$freq_corpus>=.00057)),col="blue",lty=2)
abline(lm(segments$pc~log(segments$freq_corpus)),col="purple",lty=2)

```

